{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85c917c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjet/iwazolab/NLP-Scam-Detection/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "064ff12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2c942c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sms: 0    Christa, finish your account sign-up and you c...\n",
      "1    CONGRATULATION!\\nYOUR ACCOUNT 254757547986 HAS...\n",
      "2    🙏🙏 I can do all this through him who gives me ...\n",
      "3    Hi, I'm Alice, a recruiter at VG Investments. ...\n",
      "4    TAL6FH2DN CONFIRMED, YOU HAVE RECEIVED KES. 70...\n",
      "Name: message_content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/scam_detection_labeled.csv\")\n",
    "print(\"First sms:\", df[\"message_content\"].iloc[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6cb140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'\\\\n', ' ', text)\n",
    "    text = re.sub(r'\\n+', ' ', text)\n",
    "    text = re.sub(r'[\\r\\t\\f\\v]+', ' ', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())  \n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+|[A-Za-z0-9.-]+\\.(com|org|net)\\b', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b', '', text)\n",
    "    text = re.sub(r'\\+\\d{10,12}\\b|\\+\\d{1,3}-\\d{3}-\\d{3}-\\d{4}\\b|\\b(?:\\+?254|0)(7\\d{8}|11\\d{7})\\b', '', text)\n",
    "    text = re.sub(r'\\b[A-Z0-9]{10}\\b|\\bconfirmed\\b|\\bcompleted\\b', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text) \n",
    "\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            text = text.replace(ent.text, '')\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "\n",
    "    # Second NER pass to catch missed names\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            text = text.replace(ent.text, '')\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for token in doc:\n",
    "        if not token.is_stop and token.is_alpha and len(token.text) > 2:\n",
    "            lemma = token.lemma_\n",
    "            tokens.append(lemma)\n",
    "\n",
    "\n",
    "    # Join tokens for traditional feature engineering\n",
    "    cleaned_text = \" \".join(tokens)\n",
    "\n",
    "    \n",
    "    return tokens, cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d94e65b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch BERT tokenization function\n",
    "def batch_bert_tokenize(texts, batch_size=32):\n",
    "    \"\"\"Tokenize texts in batches using BERT tokenizer.\"\"\"\n",
    "    \n",
    "    valid_texts = [text for text in texts if text.strip()]\n",
    "    if not valid_texts:\n",
    "        raise ValueError(\"No valid texts to tokenize\")\n",
    "    max_len = min(max(len(tokenizer.encode(text, truncation=True, max_length=512)) for text in valid_texts), 200)\n",
    "    print(f\"Computed max_len: {max_len}\")\n",
    "\n",
    "    all_input_ids = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        # Replace empty texts with placeholder to avoid tokenizer errors\n",
    "        batch_texts = [text if text.strip() else \"placeholder\" for text in batch_texts]\n",
    "        encodings = tokenizer(\n",
    "            batch_texts,\n",
    "            return_tensors=\"np\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_len\n",
    "        )\n",
    "        # Print first batch's input IDs\n",
    "        if i == 0:\n",
    "            print(\"Sample input IDs from first batch:\", encodings[\"input_ids\"][:2])\n",
    "        all_input_ids.append(encodings[\"input_ids\"])\n",
    "    return np.concatenate(all_input_ids, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db1349cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty cleaned_text rows: 6 (1.10%)\n",
      "Messages with >200 tokens: 10\n",
      "Generating BERT tokens...\n",
      "Computed max_len: 200\n",
      "Sample input IDs from first batch: [[  101  4070  3696 29535  2232 21462 21124  7514  2644  4895  6342  5910\n",
      "  26775 20755   102     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [  101 26478  8609  9513  4070  4923 17710  2015  2047  6781  5703 17710\n",
      "   2015  8833  2378 12816  2377   102     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]]\n",
      "Preprocessed data saved to 'preprocessed_scam_data.pkl'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "preprocessed_results = df[\"message_content\"].apply(preprocess_text)\n",
    "\n",
    "# Split results into tokens and cleaned_text\n",
    "df[\"tokens\"] = preprocessed_results.apply(lambda x: x[0])\n",
    "df[\"cleaned_text\"] = preprocessed_results.apply(lambda x: x[1])\n",
    "\n",
    "\n",
    "empty_texts = df[\"cleaned_text\"].isna() | (df[\"cleaned_text\"] == \"\")\n",
    "print(f\"Empty cleaned_text rows: {sum(empty_texts)} ({sum(empty_texts)/len(df)*100:.2f}%)\")\n",
    "\n",
    "\n",
    "long_messages = df[\"message_content\"].apply(lambda x: len(tokenizer.encode(str(x), truncation=True, max_length=512)) > 200)\n",
    "print(f\"Messages with >200 tokens: {sum(long_messages)}\")\n",
    "\n",
    "\n",
    "print(\"Generating BERT tokens...\")\n",
    "bert_input_ids = batch_bert_tokenize(df[\"cleaned_text\"].tolist())\n",
    "df[\"bert_input_ids\"] = list(bert_input_ids)\n",
    "\n",
    "\n",
    "# Save preprocessed data\n",
    "df.to_pickle(\"preprocessed_scam_data.pkl\")\n",
    "print(\"Preprocessed data saved to 'preprocessed_scam_data.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86f7fb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocessing Evaluation ---\n",
      "Remaining names detected: 65 (11.93% of rows)\n",
      "Remaining URLs: 0 (0.00% of rows)\n",
      "Non-alphabetic tokens: 0 (should be 0)\n",
      "Average tokens per message: 15.15\n",
      "Vocabulary size: 1794\n",
      "Empty token lists: 6 (1.10% of rows)\n",
      "Valid BERT input IDs (CLS/SEP): 545 (100.00% of rows)\n",
      "\n",
      "Sample of 5 rows for inspection:\n",
      "\n",
      "Row 1:\n",
      "Original: Christa, finish your account sign-up and you could make Ksh 287 hourly in Nairobi. * t.uber.com/lJ8929 Reply STOP 2 to +1 415-237-0403 to unsubscribe\n",
      "Tokens: ['account', 'sign', 'ksh', 'hourly', 'nairobi', 'reply', 'stop', 'unsubscribe']\n",
      "Cleaned Text: account sign ksh hourly nairobi reply stop unsubscribe\n",
      "BERT Decoded: account sign ksh hourly nairobi reply stop unsubscribe\n",
      "\n",
      "Row 2:\n",
      "Original: CONGRATULATION!\\nYOUR ACCOUNT 254757547986 HAS BEEN CREDITED WITH KES 62,950\\n\\nNew BONUS  Balance: KES 62,950 \\n\\nLOGIN>wekelea.com\\n \\n DEPOSIT&PLAY\n",
      "Tokens: ['congratulation', 'account', 'credit', 'kes', 'new', 'bonus', 'balance', 'kes', 'login', 'deposit', 'play']\n",
      "Cleaned Text: congratulation account credit kes new bonus balance kes login deposit play\n",
      "BERT Decoded: congratulation account credit kes new bonus balance kes login deposit play\n",
      "\n",
      "Row 3:\n",
      "Original: 🙏🙏 I can do all this through him who gives me strength. Phil 4;13 Reply with 20 To stop this message reply with STOP\n",
      "Tokens: ['give', 'strength', 'phil', 'reply', 'stop', 'message', 'reply', 'stop']\n",
      "Cleaned Text: give strength phil reply stop message reply stop\n",
      "BERT Decoded: give strength phil reply stop message reply stop\n",
      "\n",
      "Row 4:\n",
      "Original: Hi, I'm Alice, a recruiter at VG Investments. We are looking for a responsible part-time partner to join VG Investment Group. Join us and use your mobile phone for 30-60 minutes a day, with daily salary ranging from 3000-8000 KES! 👇👇👇 (must be 23 years old or above)\\nContact us via WhatsApp:+12694406502\n",
      "Tokens: ['alice', 'recruiter', 'investment', 'look', 'responsible', 'time', 'partner', 'join', 'group', 'join', 'use', 'mobile', 'phone', 'minute', 'day', 'daily', 'salary', 'range', 'ke', 'year', 'old', 'contact', 'whatsapp']\n",
      "Cleaned Text: alice recruiter investment look responsible time partner join group join use mobile phone minute day daily salary range ke year old contact whatsapp\n",
      "BERT Decoded: alice recruiter investment look responsible time partner join group join use mobile phone minute day daily salary range ke year old contact whatsapp\n",
      "\n",
      "Row 5:\n",
      "Original: TAL6FH2DN CONFIRMED, YOU HAVE RECEIVED KES. 70,350\\n\\nUSIPITWE NA HII CHEPKORIR INGIA> pepea.ke\\n\\nSHARE YAKO INAKUNGOJA LEO\\n\\nDEPOSIT 99/-PLAY&WIN\n",
      "Tokens: ['receive', 'kes', 'usipitwe', 'hii', 'chepkorir', 'ingia', 'pepea', 'share', 'yako', 'leo', 'deposit', 'play', 'win']\n",
      "Cleaned Text: receive kes usipitwe hii chepkorir ingia pepea share yako leo deposit play win\n",
      "BERT Decoded: receive kes usipitwe hii chepkorir ingia pepea share yako leo deposit play win\n",
      "\n",
      "Lemmatized tokens for first 10 messages:\n",
      "Message 1 tokens: ['account', 'sign', 'ksh', 'hourly', 'nairobi', 'reply', 'stop', 'unsubscribe']\n",
      "Message 2 tokens: ['congratulation', 'account', 'credit', 'kes', 'new', 'bonus', 'balance', 'kes', 'login', 'deposit', 'play']\n",
      "Message 3 tokens: ['give', 'strength', 'phil', 'reply', 'stop', 'message', 'reply', 'stop']\n",
      "Message 4 tokens: ['alice', 'recruiter', 'investment', 'look', 'responsible', 'time', 'partner', 'join', 'group', 'join', 'use', 'mobile', 'phone', 'minute', 'day', 'daily', 'salary', 'range', 'ke', 'year', 'old', 'contact', 'whatsapp']\n",
      "Message 5 tokens: ['receive', 'kes', 'usipitwe', 'hii', 'chepkorir', 'ingia', 'pepea', 'share', 'yako', 'leo', 'deposit', 'play', 'win']\n",
      "Message 6 tokens: ['receive', 'new', 'year', 'prize', 'facebook', 'total', 'usd', 'facebook', 'user', 'apologize', 'receive', 'prize', 'baht', 'promise', 'receive', 'today', 'send', 'mobile', 'number', 'payment', 'detail']\n",
      "Message 7 tokens: ['shall', 'pass', 'god', 'difficult', 'time', 'land', 'plenty', 'reply', 'receiver', 'stop', 'receive', 'message', 'reply', 'stop']\n",
      "Message 8 tokens: ['dear', 'tenant', 'rent', 'payment', 'mpesa', 'secure', 'problem', 'regular', 'rent', 'account', 'pay', 'time', 'payment', 'accept']\n",
      "Message 9 tokens: ['notice', 'dear', 'tenant', 'effective', 'today', 'rent', 'pay', 'new', 'bank', 'account', 'pay', 'bill', 'account', 'number', 'kindly', 'comply', 'regard']\n",
      "Message 10 tokens: ['congratulation', 'receive', 'mpesa', 'ref', 'crash', 'signup', 'hapa', 'ucheze', 'upate', 'message', 'kama', 'hii', 'stop']\n",
      "\n",
      "Sample BERT input IDs for first 2 messages:\n",
      "Message 1 BERT input IDs: [  101  4070  3696 29535  2232 21462 21124  7514  2644  4895] ...\n",
      "Message 2 BERT input IDs: [  101 26478  8609  9513  4070  4923 17710  2015  2047  6781] ...\n"
     ]
    }
   ],
   "source": [
    "# Evaluate preprocessing\n",
    "print(\"\\n--- Preprocessing Evaluation ---\")\n",
    "\n",
    "# 1. Noise Removal\n",
    "docs = list(nlp.pipe(df[\"cleaned_text\"].fillna(\"\")))\n",
    "remaining_names = sum(1 for doc in docs for ent in doc.ents if ent.label_ == \"PERSON\")\n",
    "print(f\"Remaining names detected: {remaining_names} ({remaining_names/len(df)*100:.2f}% of rows)\")\n",
    "\n",
    "url_pattern = r'http\\S+|www\\S+|https\\S+'\n",
    "urls_left = sum(1 for text in df[\"cleaned_text\"].fillna(\"\") if re.search(url_pattern, text))\n",
    "print(f\"Remaining URLs: {urls_left} ({urls_left/len(df)*100:.2f}% of rows)\")\n",
    "\n",
    "non_alpha_tokens = sum(1 for tokens in df[\"tokens\"] for token in tokens if not token.isalpha())\n",
    "print(f\"Non-alphabetic tokens: {non_alpha_tokens} (should be 0)\")\n",
    "\n",
    "# 2. Token Quality\n",
    "avg_tokens = df[\"tokens\"].apply(len).mean()\n",
    "vocab = len(set(token for tokens in df[\"tokens\"] for token in tokens))\n",
    "print(f\"Average tokens per message: {avg_tokens:.2f}\")\n",
    "print(f\"Vocabulary size: {vocab}\")\n",
    "\n",
    "# 3. Data Retention\n",
    "empty_tokens = sum(1 for tokens in df[\"tokens\"] if not tokens)\n",
    "print(f\"Empty token lists: {empty_tokens} ({empty_tokens/len(df)*100:.2f}% of rows)\")\n",
    "\n",
    "# 4. BERT Tokenization\n",
    "valid_bert = sum(1 for ids in df[\"bert_input_ids\"] if ids[0] == 101 and any(id == 102 for id in ids))\n",
    "print(f\"Valid BERT input IDs (CLS/SEP): {valid_bert} ({valid_bert/len(df)*100:.2f}% of rows)\")\n",
    "\n",
    "# 5. Manual Inspection\n",
    "print(\"\\nSample of 5 rows for inspection:\")\n",
    "sample_df = df[[\"message_content\", \"tokens\", \"cleaned_text\"]].head(5)\n",
    "for i, row in sample_df.iterrows():\n",
    "    bert_decoded = tokenizer.decode(df[\"bert_input_ids\"][i], skip_special_tokens=True)\n",
    "    print(f\"\\nRow {i+1}:\")\n",
    "    print(f\"Original: {row['message_content']}\")\n",
    "    print(f\"Tokens: {row['tokens']}\")\n",
    "    print(f\"Cleaned Text: {row['cleaned_text']}\")\n",
    "    print(f\"BERT Decoded: {bert_decoded}\")\n",
    "\n",
    "# Print tokens for the first 10 rows\n",
    "print(\"\\nLemmatized tokens for first 10 messages:\")\n",
    "for i, tokens in enumerate(df[\"tokens\"][:10]):\n",
    "    print(f\"Message {i+1} tokens:\", tokens)\n",
    "\n",
    "# Print sample BERT input IDs\n",
    "print(\"\\nSample BERT input IDs for first 2 messages:\")\n",
    "for i in range(min(2, len(df))):\n",
    "    print(f\"Message {i+1} BERT input IDs:\", df[\"bert_input_ids\"][i][:10], \"...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
