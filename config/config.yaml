data:
  raw_path: /home/sjet/iwazolab/NLP-Scam-Detection/data/raw/scam_detection_labeled.csv  # Input dataset
  processed_path: /home/sjet/iwazolab/NLP-Scam-Detection/data/processed/scam_preprocessed.csv  # Preprocessed output

run:
  tag: scam_run  # Run identifier
  timestamp: true  # Append timestamp to output filenames

output:
  logs_path: /home/sjet/iwazolab/NLP-Scam-Detection/outputs/logs/  # Directory for logs
  metrics_path: /home/sjet/iwazolab/NLP-Scam-Detection/outputs/metrics/  # Directory for metrics JSON

preprocessing:
  input_path: /home/sjet/iwazolab/NLP-Scam-Detection/data/raw/scam_detection_labeled.csv  # Same as data.raw_path
  output_path: /home/sjet/iwazolab/NLP-Scam-Detection/data/processed/scam_preprocessed.csv  # Same as data.processed_path
  sheng_keywords: ["mambo", "poa", "sawa", "vipi", "noma", "fiti", "shikisha"]  # Kiswahili Sheng keywords
  phone_regex: r"\+\d{10,12}\b|\+\d{1,3}-\d{3}-\d{3}-\d{4}\b|\b(?:\+?254|0)(7\d{8}|11\d{7})\b"  # Phone number patterns

features:
  method: all  # Extract both tfidf and bert features
  max_features: 5000  # Max TF-IDF features
  tfidf_features_dir: /home/sjet/iwazolab/NLP-Scam-Detection/data/features/tfidf/  # TF-IDF features
  tfidf_vectorizer_dir: /home/sjet/iwazolab/NLP-Scam-Detection/data/features/tfidf/  # TF-IDF vectorizer
  labels_dir: /home/sjet/iwazolab/NLP-Scam-Detection/data/features/labels/  # Labels for train/test
  bert_max_length: 128  # Max sequence length for BERT
  bert_features_dir: /home/sjet/iwazolab/NLP-Scam-Detection/data/features/bert/  # BERT features

model:
  type: all  # Train all models: logistic_regression, xgboost, bert
  save_path: /home/sjet/iwazolab/NLP-Scam-Detection/outputs/models/  # Directory for saved models
  test_split: 0.2  # Test set proportion
  hyperparams:
    C: 1.0  # Logistic Regression regularization
    max_iter: 1000  # Logistic Regression max iterations
    n_estimators: 100  # XGBoost trees
    learning_rate: 0.1  # XGBoost learning rate
    max_depth: 6  # XGBoost tree depth
    epochs: 3  # BERT training epochs
    batch_size: 8  # BERT batch size